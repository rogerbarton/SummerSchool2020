{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Euclidean Distance Matrix with NumPy\n",
    "\n",
    "In this notebook we implement two functions to compute the Euclidean distance matrix. We use a simple algebra trick that makes possible to write the function in a completely vectorized way in terms of optimized NumPy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know bradcasting. Let's use it to implement a function that calculates the Euclidean distance matrix of an array of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_broadcast(x, y):\n",
    "    \"\"\"Euclidean square distance matrix.\n",
    "    \n",
    "    Inputs:\n",
    "    x: (N, m) numpy array\n",
    "    y: (N, m) numpy array\n",
    "    \n",
    "    Ouput:\n",
    "    (N, N) Euclidean square distance matrix:\n",
    "    r_ij = (x_ij - y_ij)^2\n",
    "    \"\"\"\n",
    "    diff = x[:, np.newaxis, :] - y[np.newaxis, :, :]\n",
    "\n",
    "    return (diff * diff).sum(axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> Question </mark>: At this point you are starting to get acquainted with the `numpy.ndarray`s and it's memory managment. Could you analyse advantages and possible drawbacks of the `euclidean_broadcast` function? Write a positive and a negative point about it.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider now a more sophisticated implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_trick(x, y):\n",
    "    \"\"\"Euclidean square distance matrix.\n",
    "    \n",
    "    Inputs:\n",
    "    x: (N, m) numpy array\n",
    "    y: (N, m) numpy array\n",
    "    \n",
    "    Ouput:\n",
    "    (N, N) Euclidean square distance matrix:\n",
    "    r_ij = (x_ij - y_ij)^2\n",
    "    \"\"\"\n",
    "    x2 = np.einsum('ij,ij->i', x, x)[:, np.newaxis]\n",
    "    y2 = np.einsum('ij,ij->i', y, y)[np.newaxis, :]\n",
    "\n",
    "    xy = np.dot(x, y.T)\n",
    "\n",
    "    return np.abs(x2 + y2 - 2. * xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `euclidean_trick` function\n",
    "\n",
    "Each element of the Euclidean distance matrix is the scalar product of the difference between two rows of the array. `euclidean_trick` takes advantage of this by doing the following\n",
    "$$\n",
    "\\sum_k {(x_{ik}-y_{ik})^2} = (\\vec{x}_i - \\vec{y}_j)\\cdot(\\vec{x}_i - \\vec{y}_j) = \\vec{x}_i\\cdot\\vec{x}_i + \\vec{y}_j\\cdot\\vec{y}_j - 2\\vec{x}_i\\cdot\\vec{y}_j\n",
    "$$\n",
    "\n",
    "Fortunately, there are NumPy functions to compute each of these terms:\n",
    "\n",
    "$\\vec{x}_i\\cdot\\vec{y}_j$ $\\rightarrow$ `np.dot(x, y)` : Matrix product of $\\{\\vec{x}\\}$ and $\\{\\vec{y}\\}$\n",
    "\n",
    "$\\vec{x}_i\\cdot\\vec{x}_i$ $\\rightarrow$ `np.einsum('ij,ij->i', x, x)[:, np.newaxis]` : A $(n,1)$ vector of elements $\\sum_j x_{ij}x_{ij}$\n",
    "\n",
    "$\\vec{y}_j\\cdot\\vec{y}_j$ $\\rightarrow$ `np.einsum('ij,ij->i', y, y)[np.newaxis, :]` : A $(1,n)$ vector of elements $\\sum_j y_{ij}y_{ij}$\n",
    "\n",
    "To have all the combinations $ij$ of the sum $\\vec{x}_i\\cdot\\vec{x}_i + \\vec{y}_j\\cdot\\vec{y}_j$, we add a new axis to each of the arrays, transpose one them and add them.\n",
    "\n",
    "Let's see now how the `np.einsum` function works. `einsum` stands for Einstein summation, which is used in tensor algebra to write compact expressions without the sum symbol ($\\sum$). Within the Einstein summation notation, whenever there are repeated indexes, there is a sum over them. For instance, the expression\n",
    "$$x_{ik}y_{kj}$$\n",
    "is equivalent to\n",
    "$$\\sum_k x_{ik}y_{kj}$$\n",
    "\n",
    "`np.einsum` uses a generalized form of the Einstein summation by adding the symbol `->` to prevent summing over certain indexes. The specific operation we use here, `np.einsum('ij,ij->i', x, x)`, gives the vector\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\sum_k x_{1k}x_{1k} \\\\\n",
    "\\sum_k x_{2k}x_{2k} \\\\\n",
    " ...                \\\\\n",
    "\\sum_k x_{nk}x_{nk} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Note that the resulting vector is represented here as a column vector just for visualization purposes. It's is `(n,)` NumPy array.\n",
    "\n",
    "Let's check now step-by-step what the `euclidean_trick` function does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.49856735, 9.07330317, 5.83507426],\n",
       "       [2.31779092, 2.57644926, 6.9742852 ],\n",
       "       [6.47108862, 5.14246657, 2.04537476],\n",
       "       [2.68247323, 2.58133294, 5.35585094],\n",
       "       [7.28884481, 8.38768149, 6.42682623],\n",
       "       [1.24391379, 8.98062665, 2.85550608],\n",
       "       [1.47453894, 9.49884482, 1.4585181 ],\n",
       "       [9.82930624, 0.52015587, 3.81176234],\n",
       "       [9.84028391, 9.23037722, 7.10969376],\n",
       "       [6.92334873, 2.92270321, 8.27684602]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets generate some random data\n",
    "nsamples = 10\n",
    "nfeat = 3\n",
    "\n",
    "x = 10. * np.random.random([nsamples, nfeat])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.einsum('ij,ij->i', x, x)\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.einsum('ij,ij->i', x, x)[:, np.newaxis]\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x2 + x2.T).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use `np.dot` to perform the matrix multiplication of the full dataset by itself. We didn't use it before as alternative to `np.einsum` because it doesn't perform row by row scalar products. Instead `np.dot` expects two arrays with matching shapes $(m,n)$ and $(n,m)$ to perform a matrix multiplication.\n",
    "\n",
    "We could have used `np.einsum('ik,jk', x, x)` to perform the matrix multiplication, but we chose `np.dot(x, x.T)` instead. This is because `np.dot` is a very sophisticated too, plus it uses OpenMP threads. This results in a very fast execution.\n",
    "\n",
    "You are welcome to time them and look at the `top` command to see how `np.dot` uses multiple OpenMP threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy = np.dot(x, x.T)\n",
    "xy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, considering that the reason we are using `np.einsum` is to get rid of the loops, why didn't we use something like `(x*x).sum(axis=1)`? Let's run the next cell comparing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 µs ± 351 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "439 µs ± 91 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# let's use a larger array for timing the function calls\n",
    "nsamples = 1000\n",
    "nfeat = 300\n",
    "\n",
    "x = 10. * np.random.random([nsamples, nfeat])\n",
    "\n",
    "# it gives the same result\n",
    "np.abs(np.einsum('ij,ij->i', x, x) - (x*x).sum(axis=1)).max()\n",
    "\n",
    "# but it's not as fast as `np.einsum`\n",
    "%timeit np.einsum('ij,ij->i', x, x)\n",
    "%timeit (x*x).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a reduction with the ufunc `np.add` is also slower than `np.einsum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 µs ± 343 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.add.reduce(x*x, axis=1)\n",
    "\n",
    "# As homework, check what's the meaning of the previous line!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's time both implementations and check that they give the same result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.59 s ± 1.79 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "59.2 ms ± 18.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "nsamples = 2000\n",
    "nfeat = 50\n",
    "\n",
    "x = 10. * np.random.random([nsamples, nfeat])\n",
    "\n",
    "%timeit euclidean_broadcast(x, x)\n",
    "%timeit euclidean_trick(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.183231456205249e-12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(euclidean_broadcast(x, x) - euclidean_trick(x, x)).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> Optional question </mark> Change the implementation of `euclidean_broadcast` function to make faster using `einsum` to do the final sum. How much is the speed up? Compare it with both the original `euclidean_broadcast` and `euclidean_trick`. Check that the result is the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/edm-broadcast-einsum.py\n",
    "idean_broadcast(x, y):\n",
    "    \"\"\"Euclidean square distance matrix.\n",
    "    \n",
    "    Inputs:\n",
    "    x: (N,) numpy array\n",
    "    y: (N,) numpy array\n",
    "    \n",
    "    Ouput:\n",
    "    (N, N) Euclidean square distance matrix:\n",
    "    r_ij = x_ij^2 - y_ij^2\n",
    "    \"\"\"\n",
    "    diff = x[:, np.newaxis, :] - y[np.newaxis, :, :]\n",
    "\n",
    "    return np.einsum('ijk,ijk->ij', diff, diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The main points to take from this notebook are:\n",
    "  * NumPy is all about vectorization. Loops in python must be avoided.\n",
    "  * Always consider different vectorized implementations and compare them.\n",
    "  * Even within NumPy, some functions might bring a more significant speedup than others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniconda-ss2020",
   "language": "python",
   "name": "miniconda-ss2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
